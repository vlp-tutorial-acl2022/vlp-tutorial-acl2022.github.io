<!DOCTYPE html>
<html lang="en" class=" usyvdii idc0_336"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link href="style.css" rel="stylesheet">
    <title>ACL 2022 Tutorial - Vision-Language Pretraining: Current Trends and the Future</title>
  </head>
  <body class="vsc-initialized" style="">
    <div class="container bg-light">
        <br>
        <h2>Vision-Language Pretraining: Current Trends and the Future</h2>
        <p class="lead">An <a href="https://www.2022.aclweb.org/">ACL 2022</a> tutorial by <a href="https://www.iro.umontreal.ca/~agrawal/">Aishwarya Agrawal</a> (DeepMind, University of Montreal, Mila), <a href="http://www.damienteney.info">Damien Teney</a> (Idiap Research Institute), and <a href="http://www.aidanematzadeh.me">Aida Nematzadeh</a> (DeepMind).</p>

        <p class="lead">Slides:<br>
        &#8226; Part 1: <a href=>Classical methods</a>. (coming soon)<br>
        &#8226; Part 2: <a href=>Modern vision-language pretraining</a>. (coming soon)<br>
        &#8226; Part 3: <a href=Part3_BeyondStatisticalLearning_Teney.pdf>Beyond statistical learning</a>.
        </p>

        <p class="lead">The goal of this tutorial will be to give an overview of the ingredients needed for working on multimodal problems, particularly vision and language. We will also discuss some of the open problems and promising future directions in this area. </p>
        <p class="lead">In the last few years, there has been an increased
        interest in building multimodal (vision-language) models that are
        pretrained on larger but noisier datasets where the two modalities (e.g.,
        image and text) loosely correspond to each other (e.g., ViLBERT and
        CLIP).  Given a task (such as visual question answering), these models
        are then often fine-tuned on task-specific supervised datasets. In
        addition to the larger pretraining datasets, the transformer
        architecture and in particular self-attention applied to two modalities
        are responsible for the impressive performance of the recent pretrianed
        models on downstream tasks.
        
        This approach is appealing for a few reasons: first,  the pretraining
        datasets are often automatically curated from the Web, providing huge
        datasets with negligible collection costs. Second, we can train large
        models once, and reuse them for various tasks. Finally, these
        pretraining approach performs better or on par to previous task-specific
        models.
       
        An interesting question is whether these pretrained models -- in
        addition to their good task performance -- learn representations that
        are better at capturing the alignments between the two modalities. In
        this tutorial, we focus on recent vision-language pretraining paradigms.
        Our goal is to first provide the background on image--language datasets,
        benchmarks, and modeling innovations before the multimodal pretraining
        area. Next we discuss the different family of models used for
        vision-language pretraining, highlighting their strengths and
        shortcomings. Finally, we discuss the limits of vision-language
        pretraining through statistical learning, and the need for alternative
        approaches such as causal modeling. </p>
        
          <p class="lead">Slides and materials will be made available here
          shortly. Click <a href="mailto:nematzadeh@deepmind.com">here to
          contact us</a>.</p>
        <br>

    </div>
</body></html>
